{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/vkbvbmf94dg7fjtyqjprh7940000gn/T/ipykernel_92355/1248817349.py:50: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "# Import packages\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import logging\n",
    "import sys\n",
    "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from oepul_chat.readers.custom_pdf_reader import CustomPDFReader\n",
    "from oepul_chat.readers.custom_html_reader import CustomHTMLReader\n",
    "from oepul_chat.readers.custom_full_pdf_reader import CustomFullPDFReader\n",
    "from oepul_chat.rag_oepul_string_query_engine import RAGOEPULStringQueryEngine\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import random\n",
    "from llama_index.schema import MetadataMode\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index import download_loader\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import DocumentSummaryIndex, get_response_synthesizer\n",
    "from llama_index.prompts import PromptTemplate\n",
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KeywordTableSimpleRetriever,\n",
    ")\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "\n",
    "# Import local library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Autoreload local library\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Utilities\n",
    "def view_response(obj):\n",
    "    export = \"\"\"<div style=\"font-size: 14px;line-height: 1.5;\"><strong>Antwort</strong>:<br>\"\"\"\n",
    "    export += f\"{obj.response}<br><br>\"\n",
    "    export += \"\"\"<strong>Quellen:</strong><br>\"\"\"\n",
    "    export += \"\"\"<ul>\"\"\"\n",
    "\n",
    "    for source_node in obj.source_nodes:\n",
    "        export += f\"<li>\"\n",
    "        export += f\"<strong>{source_node.node.metadata['File Name']}</strong><br>\"\n",
    "\n",
    "        export += f\"<strong>Header Path</strong>: {source_node.node.metadata['Header Path']}<br>\"\n",
    "        export += f\"<strong>Score</strong>: {source_node.score}<br>\"\n",
    "        export += f\"<strong>Text</strong>: <i>{source_node.node.text}</i><br>\"\n",
    "        # export += f\"<strong>Metadata</strong>: {source_node.node.metadata}<br>\"\n",
    "        export += \"<br></li>\"\n",
    "\n",
    "    export += \"\"\"</ul></div>\"\"\"\n",
    "\n",
    "    display(HTML(export))\n",
    "\n",
    "\n",
    "def view_docs(docs):\n",
    "    export = \"\"\"<div style=\"font-size: 14px;line-height: 1.5;\"><ul>\"\"\"\n",
    "    for doc in docs:\n",
    "        export += f\"<li>\"\n",
    "        export += f\"<strong>{doc.metadata['File Name']}</strong><br>\"\n",
    "        export += f\"<strong>Header Path</strong>: {doc.metadata['Header Path']}<br>\"\n",
    "        export += f\"<strong>Text</strong>: <i>{doc.text}</i><br>\"\n",
    "        # export += f\"<strong>Metadata</strong>: {source_node.node.metadata}<br>\"\n",
    "        export += \"<br></li>\"\n",
    "    export += \"\"\"</ul></div>\"\"\"\n",
    "    display(HTML(export))\n",
    "\n",
    "\n",
    "def load_data(filepath, filetype, reader):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    print(f'loading data... {filepath}')\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath,\n",
    "        file_extractor={filetype: reader},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    data = loader.load_data()\n",
    "\n",
    "    # print short summary\n",
    "    print(\"Loaded {} documents\".format(len(data)))\n",
    "    print(\"First document metadata: {}\".format(data[1].metadata))\n",
    "    print(\"First document text: {}\".format(data[1].text[0:80]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logging setup\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Embedding index for all docs\n",
    "I built a CustomPDFReader which can be found in `oepul_chat/custom_pdf_reader.py` it extracts the structure out of the PDF and embeds it in the metadata field `header_path` i think this is one od the first crucial steps as it gives each text element a context in all of the files. With llama index we can then give this fiel to the retriever or the LLM or both. For the AMA Docs and Bio Austria Guide the files are loaded via the default loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data... data/OEPUL_PDF/\n",
      "Loaded 442 documents\n",
      "First document metadata: {'File Name': 'O6_14_Almbewirtschaftung_2023_04.pdf', 'Content Type': 'text', 'Header Path': 'Almbewirtschaftung/ÖPUL 2023'}\n",
      "First document text: Almbewirtschaftung STAND April 2023\n",
      "loading data... data/BIO_AUSTRIA\n",
      "Loaded 38 documents\n",
      "First document metadata: {'File Name': 'aktueller-planungsstand-zu-bio-im-oepul-2023.html', 'Content Type': 'text', 'Header Path': 'Bio-Maßnahme/Bio-Basisprämie:/Auflagen Bio-Basisprämie:/Anlage von Biodiversitäts-Flächen/Vier Möglichkeiten im Grünland:/Einzuhalten bei Biodiversitäts-Flächen am Acker:/Optionale Module (einjährig):/Acker/Grünland:/N2000/WRRL(Wasserrahmenrichtlinie):/Beitrag teilen/Bio-Maßnahme/Bio-Basisprämie:/Auflagen Bio-Basisprämie:/Anlage von Biodiversitäts-Flächen/Vier Möglichkeiten im Grünland:/Einzuhalten bei Biodiversitäts-Flächen am Acker:/Optionale Module (einjährig):/Acker/Grünland:/N2000/WRRL(Wasserrahmenrichtlinie):/Beitrag teilen', 'tag': 'p'}\n",
      "First document text: Am 13.09.2022 wurde der österreichische GAP – Strategieplan, in dem die Ausgesta\n",
      "loading data... data/AMA\n",
      "Loaded 148 documents\n",
      "First document metadata: {'page_label': '2', 'file_name': '20230131_Merkblatt_MFA2023_V3.pdf'}\n",
      "First document text: Merkblatt Mehrfachantrag 2023  Seite 2 von 53 www.eama.at  | www.ama.at   \n",
      " EDIT\n"
     ]
    }
   ],
   "source": [
    "# load all official OEPUL docs with custom PDF reader\n",
    "oepul_official_docs = load_data(\"data/OEPUL_PDF/\", \".pdf\", CustomPDFReader())\n",
    "# # load html guide from BIO Austria\n",
    "bio_austria_guide = load_data(\"data/BIO_AUSTRIA\", \".html\", CustomHTMLReader())\n",
    "# # Load rest of AMA docs with simple pdf reader\n",
    "ama_official_docs = load_data(\"data/AMA\", \".pdf\", PDFReader())\n",
    "\n",
    "# merge documents lists\n",
    "docs_list = [oepul_official_docs]# [oepul_official_docs, ama_official_docs, bio_austria_guide]  #\n",
    "documents = [doc for docs in docs_list\n",
    "             for doc in docs]\n",
    "\n",
    "# Hide certain metadata form llm and embed\n",
    "for doc in documents:\n",
    "    doc.excluded_llm_metadata_keys = [\"Content Type\", \"page_label\"]\n",
    "    doc.excluded_embed_metadata_keys = [\"Content Type\"]\n",
    "\n",
    "# write docs to pickle\n",
    "with open('data/documents.pickle', 'wb') as f:\n",
    "    pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cached docs instead\n",
    "with open('data/documents.pickle', 'rb') as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node parser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=TokenTextSplitter(chunk_size=800, chunk_overlap=20)\n",
    ")\n",
    "\n",
    "# Parse nodes from docs \n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Create VectorStoreIndex\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# Persist it for later use\n",
    "index.storage_context.persist(persist_dir=\"indices/vector_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each doc as one file\n",
    "full_docs = oepul_official_docs = load_data(\n",
    "    \"data/\", \".pdf\", CustomFullPDFReader())\n",
    "\n",
    "# Use chatgpt model\n",
    "chatgpt = OpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Create service context\n",
    "service_context = ServiceContext.from_defaults(llm=chatgpt)\n",
    "\n",
    "# Custom prompt for summarization\n",
    "summary_prompt = PromptTemplate(\n",
    "    \"Du bist ein System welches Zusammenfassungen von Maßnahmen für Landwirte in Österreich aus dem Programm Österreichischen Programm für umweltgerechte Landwirtschaft kurz OEPUL erstellt.\\n\"\n",
    "    \"Hier die Informationen zu den ÖPUL Förderungen/ Maßnahmen:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Fasse die Maßnahme zusammen, achte besonders auf die Bedingungen und Förderhöhen.\\n\"\n",
    "    \"Der Landwirt sollte schnell erfassen können, ob die Maßnahme für ihn in Frage kommt.\\n\"\n",
    "    \"Zusammenfassung: \"\n",
    ")\n",
    "\n",
    "\n",
    "# Create response synthesizer which summarizes the nodes with custom prompt\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    summary_template=summary_prompt\n",
    ")\n",
    "\n",
    "# Create document summary index\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    documents=full_docs,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Persist it for later use\n",
    "index.storage_context.persist(persist_dir=\"indices/summary_index/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oepul-chat-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
