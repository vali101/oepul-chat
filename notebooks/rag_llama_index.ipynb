{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import SimpleDirectoryReader\n",
    "import logging\n",
    "import sys\n",
    "from llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath,\n",
    "        exclude=[\"*.rst\", \"*.ipynb\", \"*.py\", \"*.bat\", \"*.txt\", \"*.png\", \"*.jpg\",\n",
    "                 \"*.jpeg\", \"*.csv\", \"*.html\", \"*.js\", \"*.css\", \"*.pdf\", \"*.json\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    return loader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 27\n",
      "> [SimpleDirectoryReader] Total files added: 27\n",
      "> [SimpleDirectoryReader] Total files added: 27\n"
     ]
    }
   ],
   "source": [
    "# Load in pdfs as llama index documents\n",
    "documents = SimpleDirectoryReader(\"./data/pdfs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 283 documents\n",
      "First document metadata: {'page_label': '1', 'file_name': 'O6_10_Erosionsschutz_Wein_Obst_Hopfen_2022_12.pdf'}\n",
      "First document text: Informationsblatt ÖPUL 2023  \n",
      "Erosionsschutz  Wein, Obst und Ho pfen Seite 1 von 6 www.eama.at  | www.ama.at   \n",
      "  \n",
      " \n",
      "ÖPUL 2023  \n",
      "Erosionsschutz  Wein, Obst  und Hopfen  \n",
      "STAND Deze\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded {} documents\".format(len(documents)))\n",
    "print(\"First document metadata: {}\".format(documents[0].metadata))\n",
    "print(\"First document text: {}\".format(documents[0].text[0:180]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the documents are splitted by page, this is IMO suboptimal a hierachical split could be beneficial as these documents are always strutured simmilary.\n",
    "\n",
    "But for now we will accept this and go over to the next step which is splitting the documents into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='text-davinci-003', temperature=0, max_tokens=256)\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    ")\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=4096,\n",
    "    num_output=256,\n",
    "    chunk_overlap_ratio=0.1,\n",
    "    chunk_size_limit=None\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=node_parser,\n",
    "    prompt_helper=prompt_helper\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Welche Maßnahme trägt zur Verringerung der Treibhausgasemission bei?\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oepul-chat-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
